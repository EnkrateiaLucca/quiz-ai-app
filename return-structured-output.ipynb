{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"langchain[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "- Answer\n",
    "    - source 1 (small piece of the pdf document)\n",
    "    - source 2 (small piece of the pdf document)\n",
    "    - source 3 (small piece of the pdf document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-11 11:24:40--  https://arxiv.org/pdf/2212.14024.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.195.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1268099 (1,2M) [application/pdf]\n",
      "Saving to: ‘2212.14024.pdf’\n",
      "\n",
      "2212.14024.pdf      100%[===================>]   1,21M  --.-KB/s    in 0,04s   \n",
      "\n",
      "2024-03-11 11:24:40 (33,4 MB/s) - ‘2212.14024.pdf’ saved [1268099/1268099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2212.14024.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./rag-intro-paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata[\"page_chunk\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents, embeddings, collection_name=\"structured-answer-from-pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12a3f5540>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"structured-answer-from-pdf\",\n",
    "    \"Query a retriever to get information about a rag pipeline in NLP intensive tasks from a pdf.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Final response to the question being asked\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"The final answer to respond to the user\")\n",
    "    sources: List[int] = Field(\n",
    "        description=\"List of page chunks that contain answer to the question. Only include a page chunk if it contains relevant information\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Loop\n",
    "\n",
    "Run time of an agent involves:\n",
    "\n",
    "while agent is running:\n",
    "   1. get input from user\n",
    "   2. return response or make a fucntion call (take an action)\n",
    "   3. If function call: \n",
    "       3.1 observation\n",
    "   4. function_of_observation(observation):\n",
    "   5. repet from step 2   \n",
    "\n",
    "- Handling erorrs\n",
    "- parsing  output from functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.agents import AgentActionMessageLog, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(output):\n",
    "    # If no function was invoked, return to user\n",
    "    if \"function_call\" not in output.additional_kwargs:\n",
    "        return AgentFinish(return_values={\"output\": output.content}, log=output.content)\n",
    "\n",
    "    # Parse out the function call\n",
    "    function_call = output.additional_kwargs[\"function_call\"]\n",
    "    name = function_call[\"name\"]\n",
    "    inputs = json.loads(function_call[\"arguments\"])\n",
    "\n",
    "    # If the Response function was invoked, return to the user with the function inputs\n",
    "    if name == \"Answer\":\n",
    "        return AgentFinish(return_values=inputs, log=str(function_call))\n",
    "    # Otherwise, return an agent action\n",
    "    else:\n",
    "        return AgentActionMessageLog(\n",
    "            tool=name, tool_input=inputs, log=\"\", message_log=[output]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        \n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_functions([retriever_tool, Answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCEL is a declarative expression language that allows for easy composition of chains with streaming support, async capabilities, optimized parallel execution, and configurable retries and fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        # Format agent scratchpad from intermediate steps\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | parse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(tools=[retriever_tool], agent=agent, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[36;1m\u001b[1;3mAs a general framework, DSP can express ideas like self-ask\n",
      "and many other, more sophisticated pipelines as we discuss\n",
      "in the present section. More importantly, DSP offers a num-\n",
      "ber of intrinsic advantages that lead to large empirical gains:\n",
      "80%–290% over self-ask. For instance, DSP programs are\n",
      "deeply modular, which among other things means that DSP\n",
      "programs will annotate and construct their own demonstra-\n",
      "tions. Thus, they can be developed without labeling any\n",
      "of the intermediate transformations (e.g., the queries gener-\n",
      "ated). In addition, the LM prompts constructed by DSP get\n",
      "automatically updated to align with the training data and re-\n",
      "trieval corpus provided. In contrast, approaches like self-ask\n",
      "rely on a hand-written prompt with hard-coded examples.\n",
      "Moreover, DSP assigns the control ﬂow to an explicit pro-\n",
      "gram and facilitates design patterns that invoke the LM(or\n",
      "RM) to conduct small transformations. This allows us to\n",
      "build steps that are dedicated to generating one or more re-\n",
      "trieval queries, summarizing multiple passages per hop, and\n",
      "answering questions. These steps are individually simpler\n",
      "than the self-ask prompt, yet our multi-hop DSP program\n",
      "deliberately composes them to build richer pipelines that are\n",
      "thus more reliable. In contrast, self-ask delegates the con-\n",
      "trol ﬂow to the LMcompletions, maintaining state within\n",
      "the prompt itself and intercepting follow-up questions to\n",
      "conduct search. We ﬁnd that this paradigm leads to a “self-\n",
      "distraction” problem (§3.5) that DSP programs avoid.\n",
      "Fusing Retrieval Results For improved recall and robust-\n",
      "ness, we can also fuse the retrieval across multiple gen-\n",
      "erated queries. Fusion has a long history in information\n",
      "\n",
      "DEMONSTRATE –SEARCH –PREDICT :\n",
      "Composing retrieval and language models for knowledge-intensive NLP\n",
      "Omar Khattab1Keshav Santhanam1Xiang Lisa Li1David Hall1\n",
      "Percy Liang1Christopher Potts1Matei Zaharia1\n",
      "Abstract\n",
      "Retrieval-augmented in-context learning has\n",
      "emerged as a powerful approach for addressing\n",
      "knowledge-intensive tasks using frozen language\n",
      "models (LM) and retrieval models (RM). Exist-\n",
      "ing work has combined these in simple “retrieve-\n",
      "then-read” pipelines in which the RM retrieves\n",
      "passages that are inserted into the LM prompt.\n",
      "To begin to fully realize the potential of frozen\n",
      "LMs and RMs, we propose DEMONSTRATE –\n",
      "SEARCH –PREDICT (DSP ), a framework that re-\n",
      "lies on passing natural language texts in sophisti-\n",
      "cated pipelines between an LM and an RM. DSP\n",
      "can express high-level programs that bootstrap\n",
      "pipeline-aware demonstrations, search for rele-\n",
      "vant passages, and generate grounded predictions,\n",
      "systematically breaking down problems into small\n",
      "transformations that the LM and RM can handle\n",
      "more reliably. We have written novel DSP pro-\n",
      "grams for answering questions in open-domain,\n",
      "multi-hop, and conversational settings, establish-\n",
      "ing in early evaluations new state-of-the-art in-\n",
      "context learning results and delivering 37–120%,\n",
      "8–39%, and 80–290% relative gains against the\n",
      "vanilla LM (GPT-3.5), a standard retrieve-then-\n",
      "read pipeline, and a contemporaneous self-ask\n",
      "pipeline, respectively. We release DSP athttps:\n",
      "//github.com/stanfordnlp/dsp .\n",
      "1. Introduction\n",
      "In-context learning adapts a frozen language model (LM) to\n",
      "tasks by conditioning the LM on a textual prompt including\n",
      "task instructions and a few demonstrating examples (Mc-\n",
      "Cann et al., 2018; Radford et al., 2019; Brown et al., 2020).\n",
      "For knowledge-intensive tasks such as question answering,\n",
      "fact checking, and information-seeking dialogue, retrieval\n",
      "models (RM) are increasingly used to augment prompts\n",
      "1Stanford University . Correspondence to:\n",
      "Omar Khattab <okhattab@cs.stanford.edu >.\n",
      "Preprint .\n",
      "How many storeys are in the castle David Gregory inherited?\n",
      "LM:Castle Gregory has three storeys.❌Hallucinates \n",
      "a fictitious castle\n",
      "RM: “St. Gregory Hotel is a nine-floor boutique hotel in D.C...”\n",
      "LM: St. Gregory Hotel has nine storeys.❌Retrieves a \n",
      "different building\n",
      "LM: “Which castle did David Gregory inherit?”\n",
      "RM: “David Gregory inherited Kinnairdy Castle in 1664...”\n",
      "LM: “How many storyes does Kinnairdy Castle have?”\n",
      "RM: “Kinnairdy Castle is a tower house, having five storeys…”\n",
      "LM: Kinnairdy Castle has fivestoreys.Vanilla LM\n",
      "Retrieve-\n",
      "then-Read\n",
      "Multi-Hop\n",
      "DSP ProgramFigure 1. A comparison between three systems based on GPT-\n",
      "3.5 (text-davinci-002 ). On its own, the LM often makes false\n",
      "assertions. An increasingly popular retrieve-then-read pipeline\n",
      "fails when simple search can’t ﬁnd an answer. In contrast, a task-\n",
      "aware DSP program successfully decomposes the problem and\n",
      "produces a correct response. Texts edited for presentation.\n",
      "with relevant information from a large corpus (Lazaridou\n",
      "et al., 2022; Press et al., 2022; Khot et al., 2022).\n",
      "Recent work has shown such retrieval-augmented in-context\n",
      "learning to be effective in simple “retrieve-then-read”\n",
      "pipelines: a query is fed to the RM and the retrieved pas-\n",
      "sages become part of a prompt that provides context for\n",
      "the LM to use in its response. In this work, we argue that\n",
      "the fact that both LMs and RMs consume (and generate or\n",
      "retrieve) natural language texts creates an opportunity for\n",
      "much more sophisticated interactions between them. Fully\n",
      "realizing this would be transformative: frozen LMs and\n",
      "RMs could serve as infrastructure across tasks, enabling\n",
      "ML- and domain-experts alike to rapidly build grounded\n",
      "AI systems at a high level of abstraction and with lower\n",
      "deployment overheads and annotation costs.\n",
      "Figure 1 begins to illustrate the power of retrieval-\n",
      "augmented in-context learning, but also the limitations of\n",
      "“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,\n",
      "2022). Our query is “How many storeys are in the castle\n",
      "\n",
      "DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\n",
      "same train/validation/test splits as in Karpukhin et al. (2020)\n",
      "and Khattab et al. (2021b).\n",
      "Table 1 reports the answer EM and F1. The task-aware DSP\n",
      "program achieves 36.6% EM, outperforming the vanilla LM\n",
      "baseline by 126% EM relative gains. This indicates the im-\n",
      "portance of grounding the LM’s predictions in retrieval, and\n",
      "it shows that state-of-the-art retrievers like ColBERTv2 have\n",
      "the capacity to do so off-the-shelf. The proposed DSP pro-\n",
      "gram also achieves relative gains of 8% in EM and 6% in F1\n",
      "over the retrieve-then-read pipeline, highlighting that non-\n",
      "trivial gains are possible by aggregating information across\n",
      "several retrieved passages as we do with self-consistency.\n",
      "These in-context learning results are competitive with a\n",
      "number of popular ﬁne-tuned systems. For instance, on\n",
      "the Open-SQuAD test set, DPR achieves 29.8% EM, well\n",
      "below our 16-shot DSP program. On the Open-SQuAD\n",
      "dev set, the powerful Fusion-in-Decoder (Izacard & Grave,\n",
      "2020) “base” approach achieves approximately 36% (i.e.,\n",
      "very similar quality to our system) when invoked with ﬁve\n",
      "retrieved passages. Nonetheless, with the default setting\n",
      "of reading 100 passages, their system reaches 48% EM in\n",
      "this evaluation. This may indicate that similar gains are\n",
      "possible for our DSP program if the PREDICT stage is made\n",
      "to aggregate information across many more passages.\n",
      "For comparison, we also evaluate the self-ask pipeline,\n",
      "which achieves 9.3% EM, suggesting that its ﬁxed pipeline\n",
      "is ineffective outside its default multi-hop setting. Study-\n",
      "ing a few examples of its errors reveals that it often de-\n",
      "composes questions in tangential ways and answers these\n",
      "questions instead. We refer to this behavior of the LMas\n",
      "“self-distraction”, and we believe it adds evidence in favor of\n",
      "our design decisions in DSP . To illustrate self-distraction,\n",
      "when self-ask is prompted with “When does The Kidnap-\n",
      "ping of Edgardo Mortara take place?”, it asks “What is The\n",
      "Kidnapping of Edgardo Mortara“ and then asks when it was\n",
      "published, a tangential question. Thus, self-ask answers\n",
      "“1997”, instead of the time The Kidnapping of Edgardo\n",
      "Mortara takes place (1858).\n",
      "For reference, Table 1 also reports (as No-retrieval LM\n",
      "SoTA) the concurrent in-context learning results from Si\n",
      "et al. (2022) using code-davinci-002 , who achieve 20.2%\n",
      "EM without retrieval and 34.0% EM with retrieval, albeit\n",
      "on a different sample and split of the SQuAD data. Overall,\n",
      "their approaches are very similar to the baselines we im-\n",
      "plement (vanilla LM and retrieve-then-read), though their\n",
      "retrieval-augmented approach retrieves (and concatenates\n",
      "into the prompt) 10 passages from a Wikipedia dump.\n",
      "HotPotQA We use the open-domain “fullwiki” setting\n",
      "of HotPotQA using its ofﬁcial Wikipedia 2017 “abstracts”\n",
      "corpus. The HotPotQA test set is hidden, so we reserve\n",
      "the ofﬁcial validation set for our testing. We sub-dividethe training set into 90%/10% train/validation splits. In the\n",
      "training (and thus validation) split, we keep only examples\n",
      "marked as “hard” in the original dataset, which matches the\n",
      "designation of the ofﬁcial validation and test sets.\n",
      "We report the ﬁnal answer EM and F1 in Table 1. On\n",
      "HotPotQA, the task-aware DSP program outperforms the\n",
      "baselines and existing work by very wide margins, exceed-\n",
      "ing the vanilla LM, the retrieve-then-read baseline, and the\n",
      "self-ask pipeline by 82%, 39%, and 80%, respectively, in\n",
      "EM. This highlights the effectiveness of building up more\n",
      "sophisticated programs that coordinate the LM andRM for\n",
      "the S EARCH step.\n",
      "These results may be pegged against the evaluation on Hot-\n",
      "PotQA in a number of concurrent papers. We ﬁrst compare\n",
      "with non-retrieval approaches, though our comparisons must\n",
      "be tentative due to variation in evaluation methodologies. Si\n",
      "et al. (2022) achieve 25.2% EM with CoT prompting. With\n",
      "a “recite-and-answer” technique for PaLM-62B (Chowdh-\n",
      "ery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang\n",
      "\n",
      "DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\n",
      "2. D EMONSTRATE –SEARCH –PREDICT\n",
      "We now introduce the DSP framework and show its expres-\n",
      "sive power by suggesting a number of strategies in which\n",
      "theLM andRM can come together to tackle complex prob-\n",
      "lems effectively. We show in §3 that such strategies out-\n",
      "perform existing in-context learning methods. We begin by\n",
      "discussing the LMandRM foundation modules on which\n",
      "DSP is built (§2.1) and then the datatypes and control ﬂow\n",
      "within DSP (§2.2). Subsequently, we discuss each of the\n",
      "three inference stages: DEMONSTRATE (§2.3), SEARCH\n",
      "(§2.4), and P REDICT (§2.5).\n",
      "2.1. Pretrained Modules: LM and RM\n",
      "ADSP program deﬁnes the communication between the\n",
      "language model LMand the retrieval model RM.\n",
      "Language Model We invoke a frozen language model\n",
      "LM to conditionally generate (orscore ) text. For each\n",
      "invocation, the program prepares a prompt that adapts the\n",
      "LM to a speciﬁc function (e.g., answering questions or\n",
      "generating queries). A prompt often includes instructions,\n",
      "a few demonstrations of the desired behavior, and an input\n",
      "query to be answered.\n",
      "As in Figure 2, the LM generates not only: (i)the ﬁnal\n",
      "answer to the input question (in the PREDICT stage), but also\n",
      "(ii)intermediate “hop” queries to ﬁnd useful information\n",
      "for the input question ( SEARCH ) as well as (iii)exemplar\n",
      "queries that illustrate how to produce queries for questions\n",
      "in the training set ( DEMONSTRATE ). This systematic use of\n",
      "theLMis a hallmark of DSP programs.\n",
      "Retrieval Model DSP programs also invoke a frozen re-\n",
      "trieval model RM toretrieve the top- kmost “relevant”\n",
      "text sequences for a given query . The RM canindex a\n",
      "massive set of pre-deﬁned passages for scalable search, and\n",
      "those passages can be updated without changing the retrieval\n",
      "parameters. The RM accepts free-form textual inputs and\n",
      "specializes in estimating the relevance (or similarity) of a\n",
      "text sequence to a query.\n",
      "As in Figure 2, the RM is responsible for retrieving (i)\n",
      "passages for each query generated by the LM(during the\n",
      "SEARCH stage), but also (ii)passages that are used within\n",
      "demonstrations ( DEMONSTRATE ). In the latter case, the\n",
      "RM’s contributions are less about providing directly rel-\n",
      "evant information to the input question and more about\n",
      "helping the LMadapt to the domain and task.\n",
      "Though not utilized in this example, the RM is also used in\n",
      "DSP for functions like retrieving “nearest-neighbor” demon-\n",
      "strations from task training data ( DEMONSTRATE ) and se-\n",
      "lecting well-grounded generated sequences from the LM\n",
      "(PREDICT ).2.2. Datatypes and Control Flow\n",
      "We have implemented the DSP framework in Python. The\n",
      "present section introduces the core data types and compos-\n",
      "able functions provided by the framework. We use illustra-\n",
      "tive code snippets to ground the examples, and to convey\n",
      "the power that comes from being able to express complex\n",
      "interactions between the LMandRM in simple programs.\n",
      "The Example Datatype To conduct a task, a DSP pro-\n",
      "gram manipulates one or more instances of the Example\n",
      "datatype. An Example behaves like a Python dictionary\n",
      "with multiple ﬁelds. The program is typically provided with\n",
      "a few training examples. The code snippet below illustrates\n",
      "this for multi-hop question answering.\n",
      "1from dsp import Example\n",
      "2\n",
      "3train = [ Example ( question =\" When was the discoverer\n",
      "of Palomar 4 born ?\", answer =\" 1889 \"),\n",
      "4 Example ( question =\"In which city did Akeem\n",
      "Ellis play in 2017? \", answer =\" Ellesmere Port \")]\n",
      "This snippet contains two labeled examples, each with a\n",
      "multi-hop question (e.g., “In which city did Akeem Ellis\n",
      "play in 2017?”) and its short answer (“Ellesmere Port”).\n",
      "Arbitrary keys and values are allowed within an Example ,\n",
      "though typical values are strings or lists of strings.\n",
      "In this task, we are unlikely to ﬁnd an individual passage\n",
      "that provides the answer to any question. For example, the\n",
      "ﬁrst training example can probably be resolved only by ﬁrst\n",
      "answering the question of who discovered Palomar (“Edwin\u001b[0m\u001b[32;1m\u001b[1;3m{'arguments': '{\"answer\":\"The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge-intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve-then-read pipelines or self-ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner.\",\"sources\":[2,2]}', 'name': 'Answer'}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge-intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve-then-read pipelines or self-ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner.',\n",
       " 'sources': [2, 2]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain simply the Retrieval Model for the DSP framework.\"\n",
    "output = agent_executor.invoke(\n",
    "    {\"input\": query},\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge-intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve-then-read pipelines or self-ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunk_sources = output[\"sources\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page chunk 2\n",
      "DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models\n",
      "QHow many storeys are in...\n",
      "Q In which city did Akeem \n",
      "Ellis play in 2017?\n",
      "A Ellesmere PortQ When was the discoverer of \n",
      "Palomar 4 born?\n",
      "A 1889Train\n",
      "Demonstrate\n",
      "defdemonstrate (x:Example ) -> Example :\n",
      "x.demos = annotate (x.train, attempt )\n",
      "return x\n",
      "defattempt (d:Example ):\n",
      "d= search(d)\n",
      "d= predict (d)\n",
      "if d.pred == d.answer: return d1QHow many storeys are in the castle...\n",
      "Q When was the discoverer of Palomar 4 born?\n",
      "A 1889\n",
      "Hop1 Who discovered Palomar 4?\n",
      "Psg1 Edwin Hubble discovered Palomar 4...\n",
      "Hop2 When was Edwin Powell born?\n",
      "Psg2 Edwin Powell Hubble (1889–1953) was...\n",
      "Pred 1889\n",
      "x : ExampleQ In which city did Akeem Ellis play...\n",
      "A Ellesmere Port\n",
      "... ...\n",
      "Pred Waterloo❌Demos“How many storeys are in the \n",
      "castle David Gregory inherited?”QHow many storeys are in the...\n",
      "Demos . . .\n",
      "Hop1 Which castle did David Gregory inherit?\n",
      "Psg1 David Gregory inherited Kinnairdy Castle...\n",
      "Hop2 How many storeys are in Kinnairdy Castle?\n",
      "Psg2 Kinnairdy Castle […] having five storeys...\n",
      "QHow many storeys does the...\n",
      ". . . . . .\n",
      "Pred Five storeysSearch\n",
      "defsearch(x:Example ) -> Example :\n",
      "x.hop1 =generate (hop_template)( x).pred\n",
      "x.psg1 =retrieve (x.hop1, k=1)[0]\n",
      "x.hop2 =generate (hop_template)( x).pred\n",
      "x.psg2 =retrieve (x.hop2, k=1)[0]\n",
      "return x2Predict\n",
      "defpredict (x:Example ) -> Example :\n",
      "x.context = [x.psg1, x.psg2]\n",
      "x.pred =generate (qa_template)( x).pred\n",
      "return x3\n",
      "“Five storeys”\n",
      "Figure 2. A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the\n",
      "DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.\n",
      "Learning from a resulting demonstration , the SEARCH stage decomposes the complex input question and retrieves supporting information\n",
      "over two retrieval hops. Finally, the P REDICT stage uses the demonstration and retrieved passages to answer the question.\n",
      "retrieve-then-read strategy fails because the RM cannot ﬁnd\n",
      "passages that directly answer the question.\n",
      "We introduce the DEMONSTRATE –SEARCH –PREDICT\n",
      "(DSP ) framework for in-context learning, which relies en-\n",
      "tirely on passing natural language text (and scores) be-\n",
      "tween a frozen RM and LM.DSP introduces a num-\n",
      "ber of composable functions that bootstrap training exam-\n",
      "ples ( DEMONSTRATE ), gather information from a knowl-\n",
      "edge corpus ( SEARCH ), and generate grounded outputs\n",
      "(PREDICT ), using them to systematically unify techniques\n",
      "from the retrieval-augmented NLP and the in-context learn-\n",
      "ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-\n",
      "tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan\n",
      "et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).\n",
      "We use DSP to suggest powerful strategies for knowledge-\n",
      "intensive tasks with compositions of these techniques. This\n",
      "reveals new conceptual possibilities for in-context learning\n",
      "in general (§2), and it allows us to present rich programs\n",
      "that set new state-of-the-art results (§3).\n",
      "Figure 1 shows the path that a DSP program might take to\n",
      "arrive at an answer, and Figure 2 illustrates how a deliberate\n",
      "program achieves this. Instead of asking the LMto answer\n",
      "this complex question, the program’s SEARCH stage uses the\n",
      "LMto generate a query “Which castle did David Gregory\n",
      "inherit?” The RM retrieves a passage saying Gregory inher-\n",
      "ited the Kinnairdy Castle. After a second search “hop” ﬁnds\n",
      "the castle’s number of storeys, the PREDICT stage queries\n",
      "theLM with these passages to answer the original question.\n",
      "Although this program implements behaviors such as query\n",
      "generation, it requires no hand-labeled examples of these\n",
      "intermediate transformations (i.e., of the queries and pas-\n",
      "sages of both retrieval hops). Instead, the DEMONSTRATEstage uses labeled question–answer pairs to implement a\n",
      "form of weak supervision that programmatically annotates\n",
      "the transformations invoked within SEARCH andPREDICT .\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    if doc.metadata[\"page_chunk\"] in pdf_chunk_sources:\n",
    "        print(f\"Page chunk {doc.metadata['page_chunk']}\")\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "from graphviz import Digraph\n",
    "import argparse\n",
    "\n",
    "class Node(BaseModel):\n",
    "    id: int\n",
    "    label: str\n",
    "    color: str\n",
    "\n",
    "class Edge(BaseModel):\n",
    "    source: int\n",
    "    target: int\n",
    "    label: str\n",
    "    color: str = \"black\"\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    nodes: List[Node] = Field(..., default_factory=list)\n",
    "    edges: List[Edge] = Field(..., default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def visualize_knowledge_graph(kg: KnowledgeGraph):\n",
    "    dot = Digraph(comment=\"Knowledge Graph\")\n",
    "\n",
    "    # Add nodes\n",
    "    for node in kg.nodes:\n",
    "        dot.node(str(node.id), node.label, color=node.color)\n",
    "\n",
    "    # Add edges\n",
    "    for edge in kg.edges:\n",
    "        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n",
    "\n",
    "    # Render the graph\n",
    "    display(dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge-intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve-then-read pipelines or self-ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner.',\n",
       " 'sources': [2, 2]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `answer`: Node\n",
    "- `sources`: Node\n",
    "    - 0:\n",
    "    - 1:\n",
    "    ...\n",
    "- `query`: Node\n",
    "- `answer` - `query`: Edge\n",
    "- `answer` - `0`: Edge\n",
    "- `answer` - `1`: Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_schema = \"\"\"\n",
    "- `answer`: Node\n",
    "- `sources`: Node\n",
    "    - 0:\n",
    "    - 1:\n",
    "    ...\n",
    "- `query`: Node\n",
    "- `answer` - `query`: Edge\n",
    "- `answer` - `0`: Edge\n",
    "- `answer` - `1`: Edge\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install instructor openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "def generate_graph(answer_output, structure_schema) -> KnowledgeGraph:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[\n",
    "            {   \"role\": \"system\",\n",
    "                \"content\": \"You are a parser engine for knowledge graph visualization that transforms dictionary outputs into knowledge graph objects.\\\n",
    "                    The objects in the knowledge graph should be simplifications of the concepts in the question and answer so they can be human readable.\\\n",
    "                    Always use this structure schema for your answers:\\n {structure_schema}. Make sure the knowledge graph has the question, the answer and the sources connected to the answer, each its own node.\",\n",
    "            },\n",
    "            { \n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate this output answer to a knowledge graph object:\\n {answer_output}.\",\n",
    "            }\n",
    "        ],\n",
    "        response_model=KnowledgeGraph,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = output['answer']\n",
    "sources = output['sources']\n",
    "answer_output = f\"The query: {query}.\\n\\n The answer: {answer}\\n\\n The sources: {sources}.\"\n",
    "\n",
    "knowledge_graph_object = generate_graph(answer_output, structure_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes=[Node(id=1, label='Query', color='blue'), Node(id=2, label='Explain simply the Retrieval Model for the DSP framework.', color='green'), Node(id=3, label='Answer', color='blue'), Node(id=4, label='The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge-intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve-then-read pipelines or self-ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner.', color='green'), Node(id=5, label='Sources', color='blue'), Node(id=6, label='[2, 2]', color='green')], edges=[Edge(source=1, target=2, label='query', color='black'), Edge(source=3, target=4, label='answer', color='black'), Edge(source=5, target=6, label='sources', color='black')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_graph_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"8770pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 8769.64 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 8765.64,-128.5 8765.64,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"237.14\" cy=\"-106.5\" rx=\"33.95\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.14\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Query</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"237.14\" cy=\"-18\" rx=\"237.14\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Explain simply the Retrieval Model for the DSP framework.</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.14,-88.41C237.14,-76.76 237.14,-61.05 237.14,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.64,-47.86 237.14,-37.86 233.64,-47.86 240.64,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.51\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">query</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"4582.14\" cy=\"-106.5\" rx=\"39.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"4582.14\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Answer</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"4582.14\" cy=\"-18\" rx=\"4089.52\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"4582.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">The Retrieval Model (RM) within the DSP (DEMONSTRATE – SEARCH – PREDICT) framework plays a crucial role in enhancing the capabilities of language models (LMs) for knowledge&#45;intensive tasks. The DSP framework is designed to facilitate sophisticated interactions between LMs and RMs, allowing for the construction of complex, reliable pipelines that systematically break down problems into smaller, manageable transformations. This modular approach enables DSP programs to generate retrieval queries, summarize multiple passages, and provide answers more effectively than simpler retrieve&#45;then&#45;read pipelines or self&#45;ask approaches. By assigning explicit control flow to DSP programs and leveraging the RM for both retrieving relevant passages and constructing demonstrations, DSP achieves significant empirical gains in performance. Additionally, the framework supports the fusion of retrieval results from multiple queries to improve recall and robustness, showcasing the power of combining retrieval and language models in a structured, programmatic manner.</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M4582.14,-88.41C4582.14,-76.76 4582.14,-61.05 4582.14,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"4585.64,-47.86 4582.14,-37.86 4578.64,-47.86 4585.64,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"4601.26\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">answer</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"8721.14\" cy=\"-106.5\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"8721.14\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">Sources</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"8721.14\" cy=\"-18\" rx=\"31.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"8721.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">[2, 2]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M8721.14,-88.41C8721.14,-76.76 8721.14,-61.05 8721.14,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"8724.64,-47.86 8721.14,-37.86 8717.64,-47.86 8724.64,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"8741.39\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">sources</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x169cd0460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_knowledge_graph(knowledge_graph_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "returning-structured-output",
   "language": "python",
   "name": "returning-structured-output"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
